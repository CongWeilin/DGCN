{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import load_data, process_graph_data\n",
    "from utils import package_mxl, adj_rw_norm\n",
    "from utils import sparse_mx_to_torch_sparse_tensor\n",
    "from utils import ResultRecorder\n",
    "\n",
    "from model import GCN, GCNBias, SGC, ResGCN, GCNII, APPNP, MLP\n",
    "from load_semigcn_data import load_data_gcn\n",
    "from data_loader import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import copy \n",
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from tqdm import trange\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "import pickle\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=20480000000, cuda=0, dataset='cora', dropout=0, epoch_num=300, method='GCN/ResGCN/GCNII', n_layers=2, nhid=64)\n"
     ]
    }
   ],
   "source": [
    "import sys; sys.argv=['']; del sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import argparse\n",
    "\"\"\"\n",
    "Dataset arguments\n",
    "\"\"\"\n",
    "parser = argparse.ArgumentParser(\n",
    "    description='Training GCN on Large-scale Graph Datasets')\n",
    "parser.add_argument('--dataset', type=str, default='cora',\n",
    "                    help='Dataset name: pubmed/flickr/reddit/ppi-large')\n",
    "parser.add_argument('--method', type=str, default='GCN/ResGCN/GCNII', # SGC/GCN/GCNBias/ResGCN/GCNII/APPNP\n",
    "                    help='Algorithms: seperate using slash')\n",
    "parser.add_argument('--nhid', type=int, default=64,\n",
    "                    help='Hidden state dimension')\n",
    "parser.add_argument('--epoch_num', type=int, default=300,\n",
    "                    help='Number of Epoch')\n",
    "parser.add_argument('--batch_size', type=int, default=20480000000,\n",
    "                    help='size of output node in a batch')\n",
    "parser.add_argument('--n_layers', type=int, default=2,\n",
    "                    help='Number of GCN layers')\n",
    "parser.add_argument('--dropout', type=float, default=0,\n",
    "                    help='Dropout rate')\n",
    "parser.add_argument('--cuda', type=int, default=0,\n",
    "                    help='Avaiable GPU ID')\n",
    "args = parser.parse_args()\n",
    "print(args)\n",
    "\n",
    "method = args.method.split('/')\n",
    "\n",
    "\"\"\"\n",
    "Prepare devices\n",
    "\"\"\"\n",
    "if args.cuda != -1:\n",
    "    device = torch.device(\"cuda:\" + str(args.cuda))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "if args.dataset not in ['cora', 'citeseer', 'pubmed']:\n",
    "    temp_data = load_data(args.dataset) \n",
    "else:\n",
    "    temp_data = load_data_gcn(args.dataset)\n",
    "\n",
    "adj_full, adj_train, feat_data, labels, role = process_graph_data(*temp_data)\n",
    "\n",
    "train_nodes = np.array(role['tr'])\n",
    "valid_nodes = np.array(role['va'])\n",
    "test_nodes = np.array(role['te'])\n",
    "\n",
    "data_loader = DataLoader(adj_full, train_nodes, valid_nodes, test_nodes, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def weight_constaint(model):\n",
    "#     for p in model.parameters():\n",
    "#         if p.data.norm(2) > 20:\n",
    "#             p.data = p.data / p.data.norm(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Setup datasets and models for training (multi-class use sigmoid+binary_cross_entropy, use softmax+nll_loss otherwise)\n",
    "\"\"\"\n",
    "\n",
    "if args.dataset in ['flickr', 'reddit', 'cora', 'citeseer', 'pubmed']:\n",
    "    feat_data_th = torch.FloatTensor(feat_data)\n",
    "    labels_th = torch.LongTensor(labels.argmax(1))\n",
    "    num_classes = labels_th.max().item()+1\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    multi_class=False\n",
    "elif args.dataset in ['ppi', 'ppi-large', 'amazon', 'yelp']:\n",
    "    feat_data_th = torch.FloatTensor(feat_data)\n",
    "    labels_th = torch.FloatTensor(labels)\n",
    "    num_classes = labels_th.shape[1]\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    multi_class=True\n",
    "\n",
    "feat_data_th = feat_data_th.to(device)\n",
    "labels_th = labels_th.to(device)\n",
    "\n",
    "def sgd_step(net, optimizer, feat_data, labels, train_data, device):\n",
    "    \"\"\"\n",
    "    Function to updated weights with a SGD backpropagation\n",
    "    args : net, optimizer, train_loader, test_loader, loss function, number of inner epochs, args\n",
    "    return : train_loss, test_loss, grad_norm_lb\n",
    "    \"\"\"\n",
    "    net.train()\n",
    "    epoch_loss = []\n",
    "    epoch_acc = []\n",
    "    \n",
    "    # Run over the train_loader\n",
    "    mini_batches, adj = train_data\n",
    "    for mini_batch in mini_batches:\n",
    "\n",
    "        # compute current stochastic gradient\n",
    "        optimizer.zero_grad()\n",
    "        output = net(feat_data, adj)\n",
    "        \n",
    "        loss = net.criterion(output[mini_batch], labels[mini_batch])\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "        if multi_class:\n",
    "            output[output > 0.5] = 1\n",
    "            output[output <= 0.5] = 0\n",
    "        else:\n",
    "            output = output.argmax(dim=1)\n",
    "\n",
    "        acc = f1_score(output[mini_batch].detach().cpu(), \n",
    "                       labels[mini_batch].detach().cpu(), average=\"micro\")\n",
    "        epoch_acc.append(acc)\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def inference(eval_model, feat_data, labels, test_data, device):\n",
    "    eval_model = eval_model.to(device)\n",
    "    mini_batch, adj = test_data    \n",
    "    output = eval_model(feat_data, adj)\n",
    "    loss = eval_model.criterion(output[mini_batch], labels[mini_batch]).item()\n",
    "    \n",
    "    if multi_class:\n",
    "        output[output > 0.5] = 1\n",
    "        output[output <= 0.5] = 0\n",
    "    else:\n",
    "        output = output.argmax(dim=1)\n",
    "        \n",
    "    acc = f1_score(output[mini_batch].detach().cpu(), \n",
    "                   labels[mini_batch].detach().cpu(), average=\"micro\")\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "def copy_model(model):\n",
    "    state_dict = {k:v.to('cpu') for k, v in model.named_parameters()}\n",
    "    state_dict = OrderedDict(state_dict)\n",
    "    return state_dict\n",
    "def copy_grad(model):\n",
    "    state_dict = {k:v.grad.to('cpu') for k, v in model.named_parameters()}\n",
    "    state_dict = OrderedDict(state_dict)\n",
    "    return state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train without sampling\n",
    "\"\"\"\n",
    "\n",
    "def train_model(model, data_loader, note):\n",
    "    train_model = copy.deepcopy(model).to(device)\n",
    "    \n",
    "    results = ResultRecorder(note=note)\n",
    "    \n",
    "    optimizer = optim.Adam(train_model.parameters())\n",
    "\n",
    "    tbar = trange(args.epoch_num, desc='Training Epochs')\n",
    "    for epoch in tbar:\n",
    "        # fetch train data \n",
    "        \n",
    "        sample_time_st = time.perf_counter()\n",
    "        train_data = data_loader.get_mini_batches(batch_size=args.batch_size)\n",
    "        sample_time = time.perf_counter() - sample_time_st\n",
    "        \n",
    "        compute_time_st = time.perf_counter()\n",
    "        train_loss, train_acc = sgd_step(train_model, optimizer, feat_data_th, labels_th, train_data, device)\n",
    "        compute_time = time.perf_counter() - compute_time_st\n",
    "        results.grad_norms += [copy_grad(train_model)]\n",
    "        \n",
    "        epoch_train_loss = np.mean(train_loss)\n",
    "        epoch_train_acc = np.mean(train_acc)\n",
    "\n",
    "        valid_data = data_loader.get_valid_batch()\n",
    "        epoch_valid_loss, epoch_valid_acc = inference(train_model, feat_data_th, labels_th, valid_data, device)\n",
    "        tbar.set_postfix(loss=epoch_train_loss,\n",
    "                         val_loss=epoch_valid_loss,\n",
    "                         val_score=epoch_valid_acc)\n",
    "\n",
    "        results.update(epoch_train_loss, \n",
    "                       epoch_train_acc,\n",
    "                       epoch_valid_loss, \n",
    "                       epoch_valid_acc, \n",
    "                       train_model, sample_time=sample_time, compute_time=compute_time)\n",
    "        \n",
    "        results.state_dicts += [copy_model(train_model)]\n",
    "\n",
    "\n",
    "#     test_data = data_loader.get_test_batch()\n",
    "#     epoch_test_loss, epoch_test_acc = inference(results.best_model, feat_data_th, labels_th, test_data, device)\n",
    "#     results.test_loss = epoch_test_loss\n",
    "#     results.test_acc = epoch_test_acc\n",
    "    \n",
    "#     print('Test_loss: %.4f | test_acc: %.4f' % (epoch_test_loss, epoch_test_acc))\n",
    "    \n",
    "    print('Average sampling time %.5fs, average computing time %.5fs'%\n",
    "          (np.mean(results.sample_time), np.mean(results.compute_time)))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, n_feat, n_hid, n_classes, n_layers, dropout, criterion):\n",
    "        from layers import GraphConv\n",
    "        super(GCN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hid = n_hid\n",
    "        \n",
    "        self.gcs = nn.ModuleList()\n",
    "        for _ in range(n_layers):\n",
    "            self.gcs.append(GraphConv(n_hid,  n_hid))\n",
    "        self.linear_in = nn.Linear(n_feat, n_hid)\n",
    "        self.linear_out = nn.Linear(n_hid, n_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.linear_in(x)\n",
    "        for ell in range(len(self.gcs)):\n",
    "            x = self.gcs[ell](x, adj)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        x = self.linear_out(x)\n",
    "        return x\n",
    "    \n",
    "class ResGCN(nn.Module):\n",
    "    def __init__(self, n_feat, n_hid, n_classes, n_layers, dropout, criterion):\n",
    "        from layers import GraphConv\n",
    "        super(ResGCN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hid = n_hid\n",
    "        \n",
    "        self.gcs = nn.ModuleList()\n",
    "        for _ in range(n_layers):\n",
    "            self.gcs.append(GraphConv(n_hid,  n_hid))\n",
    "        self.linear_in = nn.Linear(n_feat, n_hid)\n",
    "        self.linear_out = nn.Linear(n_hid, n_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.linear_in(x)\n",
    "        for ell in range(len(self.gcs)):\n",
    "            x_res = x.clone()\n",
    "            x = self.gcs[ell](x, adj)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout(x) + x_res\n",
    "        x = self.linear_out(x)\n",
    "        return x\n",
    "    \n",
    "class GCNII(nn.Module):\n",
    "    def __init__(self, n_feat, n_hid, n_classes, n_layers, dropout, criterion):\n",
    "        from layers import GCNIILayer\n",
    "        super(GCNII, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hid = n_hid\n",
    "        \n",
    "        self.gcs = nn.ModuleList()\n",
    "        for _ in range(n_layers):\n",
    "            self.gcs.append(GCNIILayer(n_hid,  n_hid))\n",
    "        self.linear_in = nn.Linear(n_feat, n_hid)\n",
    "        self.linear_out = nn.Linear(n_hid, n_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def get_alpha_beta(self, ell):\n",
    "        alpha = 0.9\n",
    "        beta = math.log(0.5/(ell+1)+1)\n",
    "        return alpha, beta\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        x = self.linear_in(x)\n",
    "        x_0 = x.clone()\n",
    "        for ell in range(len(self.gcs)):\n",
    "            alpha, beta = self.get_alpha_beta(ell)\n",
    "            x = self.gcs[ell](x, adj, x_0, alpha, beta)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        x = self.linear_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcs.0.linear.weight tensor(0.4865) tensor(1.0987) tensor(0.0082)\n",
      "gcs.1.linear.weight tensor(0.4945) tensor(1.1197) tensor(0.0093)\n",
      "linear_in.weight tensor(0.5737) tensor(0.6902) tensor(0.4574)\n",
      "linear_out.weight tensor(0.5544) tensor(0.7141) tensor(0.3792)\n"
     ]
    }
   ],
   "source": [
    "model = GCN(n_feat=feat_data.shape[1], \n",
    "                n_hid=args.nhid, \n",
    "                n_classes=num_classes, \n",
    "                n_layers=args.n_layers, \n",
    "                dropout=args.dropout, \n",
    "                criterion=criterion)\n",
    "\n",
    "# def get_weight_sigval(model):\n",
    "#     weight_sigval= dict()\n",
    "#     for n, p in model.named_parameters():\n",
    "#         if 'weight' in n and 'gcs' in n:\n",
    "#             U, S, V = torch.svd(p.data, compute_uv=False)\n",
    "#             weight_sigval[n] = S.max().item()\n",
    "#     return weight_sigval\n",
    "# get_weight_sigval(model)\n",
    "\n",
    "def get_sigval(model):\n",
    "    for n, p in model.named_parameters():\n",
    "        if 'weight' in n:\n",
    "            _, val, _ = torch.svd(p.data)\n",
    "            print(n, val.mean(), val.max(), val.min())\n",
    "get_sigval(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_list = []\n",
    "for repeat in range(5):\n",
    "    if 'GCN' in method:\n",
    "        model = GCN(n_feat=feat_data.shape[1], \n",
    "                    n_hid=args.nhid, \n",
    "                    n_classes=num_classes, \n",
    "                    n_layers=args.n_layers, \n",
    "                    dropout=args.dropout, \n",
    "                    criterion=criterion)\n",
    "\n",
    "        results = train_model(model, data_loader, note=\"GCN (L=%d, repeat=%d)\"%(args.n_layers, repeat))\n",
    "        results_list.append(results)\n",
    "\n",
    "    if 'ResGCN' in method:\n",
    "        model = ResGCN(n_feat=feat_data.shape[1], \n",
    "                    n_hid=args.nhid, \n",
    "                    n_classes=num_classes, \n",
    "                    n_layers=args.n_layers, \n",
    "                    dropout=args.dropout, \n",
    "                    criterion=criterion)\n",
    "\n",
    "        results = train_model(model, data_loader, note=\"ResGCN (L=%d, repeat=%d)\"%(args.n_layers, repeat))\n",
    "        results_list.append(results)\n",
    "\n",
    "    if 'GCNII' in method:\n",
    "        model = GCNII(n_feat=feat_data.shape[1], \n",
    "                    n_hid=args.nhid, \n",
    "                    n_classes=num_classes, \n",
    "                    n_layers=args.n_layers, \n",
    "                    dropout=args.dropout, \n",
    "                    criterion=criterion)\n",
    "\n",
    "        results = train_model(model, data_loader, note=\"GCNII (L=%d, repeat=%d)\"%(args.n_layers, repeat))\n",
    "        results_list.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "\n",
    "for algorithm in ['GCN', 'ResGCN', 'GCNII']:\n",
    "    color = next(axs._get_lines.prop_cycler)['color']\n",
    "    \n",
    "    train_acc = []\n",
    "    y_vals = []\n",
    "    for result in results_list:\n",
    "        if result.note.split()[0] == algorithm:\n",
    "            train_acc.append(np.array(result.train_acc_record))\n",
    "            train_loss = np.array(result.train_loss_record)\n",
    "            valid_loss = np.array(result.loss_record)\n",
    "            y_vals.append(np.abs(train_loss-valid_loss))\n",
    "\n",
    "    train_acc = np.mean(train_acc, axis=0)\n",
    "    x_stop = np.argmax(train_acc)\n",
    "\n",
    "    y_val_mean = np.mean(y_vals, axis=0)[:x_stop]\n",
    "    y_val_std = np.std(y_vals, axis=0)[:x_stop]\n",
    "    x_vals = np.arange(len(y_val_mean))\n",
    "    \n",
    "    axs.plot(x_vals, y_val_mean, label='%s (L=%d)'%(algorithm, args.n_layers), color=color)\n",
    "    axs.fill_between(x_vals, y_val_mean-y_val_std, y_val_mean+y_val_std ,alpha=0.3, color=color)\n",
    "    \n",
    "plt.title('Generalization gap / Iters')\n",
    "axs.set_xlabel('Iters')\n",
    "axs.set_ylabel('| Train Error - Valid Error |')\n",
    "\n",
    "axs.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('generalization gap.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "\n",
    "for algorithm in ['GCN', 'ResGCN', 'GCNII']:\n",
    "    color = next(axs._get_lines.prop_cycler)['color']\n",
    "    \n",
    "    y_vals = []\n",
    "    for result in results_list:\n",
    "        if result.note.split()[0] == algorithm:\n",
    "            y_vals.append(np.array(result.train_acc_record))\n",
    "            \n",
    "    y_val_mean = np.mean(y_vals, axis=0)\n",
    "    y_val_std = np.std(y_vals, axis=0)\n",
    "    x_vals = np.arange(len(y_val_mean))\n",
    "\n",
    "    axs.plot(x_vals, y_val_mean, label='Train: %s (L=%d)'%(algorithm, args.n_layers), color=color)\n",
    "    axs.fill_between(x_vals, y_val_mean-y_val_std, y_val_mean+y_val_std ,alpha=0.3, color=color)\n",
    "    \n",
    "    y_vals = []\n",
    "    for result in results_list:\n",
    "        if result.note.split()[0] == algorithm:\n",
    "            y_vals.append(np.array(result.acc_record))\n",
    "            \n",
    "    y_val_mean = np.mean(y_vals, axis=0)\n",
    "    y_val_std = np.std(y_vals, axis=0)\n",
    "    x_vals = np.arange(len(y_val_mean))\n",
    "\n",
    "    axs.plot(x_vals, y_val_mean, label='Valid: %s (L=%d)'%(algorithm, args.n_layers), color=color, linestyle='--')\n",
    "    axs.fill_between(x_vals, y_val_mean-y_val_std, y_val_mean+y_val_std ,alpha=0.3, color=color, linestyle='--')\n",
    "    \n",
    "    \n",
    "plt.title('F1-score / Iters')\n",
    "axs.set_xlabel('Iters')\n",
    "axs.set_ylabel('F1-score')\n",
    "\n",
    "axs.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('gcn_resnet_gcnii_f1_score.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
