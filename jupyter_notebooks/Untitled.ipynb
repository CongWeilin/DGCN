{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import load_data, process_graph_data\n",
    "from utils import package_mxl, adj_rw_norm\n",
    "from utils import sparse_mx_to_torch_sparse_tensor\n",
    "from utils import ResultRecorder\n",
    "\n",
    "from model import GCN, GCNBias, SGC, ResGCN, GCNII, APPNP\n",
    "from layers import GraphConv\n",
    "from load_semigcn_data import load_data_gcn\n",
    "from data_loader import DataLoader\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "import copy \n",
    "import time\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.argv=['']; del sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=99999, cuda=0, dataset='citeseer', dropout=0, epoch_num=500, method='GCN/ResGCN/GCNII', n_layers=10, nhid=64)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\"\"\"\n",
    "Dataset arguments\n",
    "\"\"\"\n",
    "parser = argparse.ArgumentParser(\n",
    "    description='Training GCN on Large-scale Graph Datasets')\n",
    "parser.add_argument('--dataset', type=str, default='citeseer',\n",
    "                    help='Dataset name: pubmed/flickr/reddit/ppi-large')\n",
    "parser.add_argument('--method', type=str, default='GCN/ResGCN/GCNII', # SGC/GCN/GCNBias/ResGCN/GCNII/APPNP\n",
    "                    help='Algorithms: seperate using slash')\n",
    "parser.add_argument('--nhid', type=int, default=64,\n",
    "                    help='Hidden state dimension')\n",
    "parser.add_argument('--epoch_num', type=int, default=500,\n",
    "                    help='Number of Epoch')\n",
    "parser.add_argument('--batch_size', type=int, default=99999,\n",
    "                    help='size of output node in a batch')\n",
    "parser.add_argument('--n_layers', type=int, default=10,\n",
    "                    help='Number of GCN layers')\n",
    "parser.add_argument('--dropout', type=float, default=0,\n",
    "                    help='Dropout rate')\n",
    "parser.add_argument('--cuda', type=int, default=0,\n",
    "                    help='Avaiable GPU ID')\n",
    "args = parser.parse_args()\n",
    "print(args)\n",
    "\n",
    "method = args.method.split('/')\n",
    "\n",
    "\"\"\"\n",
    "Prepare devices\n",
    "\"\"\"\n",
    "if args.cuda != -1:\n",
    "    device = torch.device(\"cuda:\" + str(args.cuda))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "if args.dataset not in ['cora', 'citeseer', 'pubmed']:\n",
    "    temp_data = load_data(args.dataset) \n",
    "else:\n",
    "    temp_data = load_data_gcn(args.dataset)\n",
    "\n",
    "adj_full, adj_train, feat_data, labels, role = process_graph_data(*temp_data)\n",
    "\n",
    "train_nodes = np.array(role['tr'])\n",
    "valid_nodes = np.array(role['va'])\n",
    "test_nodes = np.array(role['te'])\n",
    "\n",
    "data_loader = DataLoader(adj_full, train_nodes, valid_nodes, test_nodes, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proj_mat(data_loader):\n",
    "    adj_mat = data_loader.adj_mat.toarray()\n",
    "    num_nodes = data_loader.num_nodes\n",
    "    n_components, components_label = connected_components(adj_mat)\n",
    "    E_mat = np.zeros((n_components, num_nodes))\n",
    "    for node_i in range(num_nodes):\n",
    "        deg = adj_mat[node_i, :].sum()\n",
    "        E_mat[components_label[node_i], node_i] = 1/np.sqrt(deg)\n",
    "    E_mat = (E_mat.T/E_mat.sum(axis=1)).T\n",
    "\n",
    "    P_mat = np.matmul(E_mat.T, np.linalg.inv(E_mat.dot(E_mat.T))).dot(E_mat)\n",
    "    F_mat = np.eye(P_mat.shape[0]) - P_mat\n",
    "    return F_mat\n",
    "\n",
    "def compute_dM(model, F_mat, feat_data_th, data_loader):\n",
    "    hiddens = model.fetch_hiddens(feat_data_th, data_loader.lap_tensor)\n",
    "    dM = [np.linalg.norm(F_mat.dot(hidden)) for hidden in hiddens]\n",
    "    # dM = np.array(dM)/dM[0]\n",
    "    return dM\n",
    "\n",
    "def get_weight_norms(model):\n",
    "    weight_norms = dict()\n",
    "    for n, p in model.named_parameters():\n",
    "        if 'weight' in n and 'gcs' in n:\n",
    "            weight_norms[n] = p.data.norm(2).item()\n",
    "    return weight_norms\n",
    "\n",
    "def get_weight_sigval(model):\n",
    "    weight_sigval= dict()\n",
    "    for n, p in model.named_parameters():\n",
    "        if 'weight' in n and 'gcs' in n:\n",
    "            U, S, V = torch.svd(p.data, compute_uv=False)\n",
    "            weight_sigval[n] = S.max().item()\n",
    "    return weight_sigval\n",
    "\n",
    "def get_grad_norms(model):\n",
    "    grad_norms = dict()\n",
    "    for n, p in model.named_parameters():\n",
    "        grad_norms[n] = p.grad.data.norm(2).item()\n",
    "    return grad_norms\n",
    "        \n",
    "def compute_hidden_dist(model, feat_data_th, data_loader):\n",
    "    unique_labels = np.arange(num_classes)\n",
    "    class_to_indices = dict()\n",
    "    for i in unique_labels:\n",
    "        class_to_indices[i] = np.where(np.argmax(labels, axis=1)==i)[0]\n",
    "\n",
    "    inner_class_dist_list = []\n",
    "    cross_class_dist_list = []\n",
    "    for hiddens in model.fetch_hiddens(feat_data_th, data_loader.lap_tensor):\n",
    "        hiddens_norm = np.linalg.norm(hiddens)\n",
    "\n",
    "        inner_class_dist = []\n",
    "        for i in unique_labels:\n",
    "            indices = class_to_indices[i]\n",
    "            dists = pairwise_distances(hiddens[indices])\n",
    "            inner_class_dist.append(dists.mean())\n",
    "        inner_class_dist_list += [np.mean(inner_class_dist)/hiddens_norm]\n",
    "\n",
    "        cross_class_dist = []\n",
    "        all_nodes_indices = np.arange(len(labels))\n",
    "        for i in unique_labels:\n",
    "            indices = class_to_indices[i]\n",
    "            other_indices = np.setdiff1d(all_nodes_indices, indices)\n",
    "            dists = pairwise_distances(hiddens[indices], hiddens[other_indices])\n",
    "            cross_class_dist.append(dists.mean())\n",
    "        cross_class_dist_list += [np.mean(cross_class_dist)/hiddens_norm]\n",
    "    return inner_class_dist_list, cross_class_dist_list\n",
    "\n",
    "\n",
    "F_mat = get_proj_mat(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Setup datasets and models for training (multi-class use sigmoid+binary_cross_entropy, use softmax+nll_loss otherwise)\n",
    "\"\"\"\n",
    "\n",
    "if args.dataset in ['flickr', 'reddit', 'cora', 'citeseer', 'pubmed']:\n",
    "    feat_data_th = torch.FloatTensor(feat_data)\n",
    "    labels_th = torch.LongTensor(labels.argmax(1))\n",
    "    num_classes = labels_th.max().item()+1\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    multi_class=False\n",
    "elif args.dataset in ['ppi', 'ppi-large', 'amazon', 'yelp']:\n",
    "    feat_data_th = torch.FloatTensor(feat_data)\n",
    "    labels_th = torch.FloatTensor(labels)\n",
    "    num_classes = labels_th.shape[1]\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    multi_class=True\n",
    "\n",
    "feat_data_th = feat_data_th.to(device)\n",
    "labels_th = labels_th.to(device)\n",
    "\n",
    "def sgd_step(net, optimizer, feat_data, labels, train_data, device):\n",
    "    \"\"\"\n",
    "    Function to updated weights with a SGD backpropagation\n",
    "    args : net, optimizer, train_loader, test_loader, loss function, number of inner epochs, args\n",
    "    return : train_loss, test_loss, grad_norm_lb\n",
    "    \"\"\"\n",
    "    net.train()\n",
    "    epoch_loss = []\n",
    "    epoch_acc = []\n",
    "    \n",
    "    # Run over the train_loader\n",
    "    mini_batches, adj = train_data\n",
    "    for mini_batch in mini_batches:\n",
    "\n",
    "        # compute current stochastic gradient\n",
    "        optimizer.zero_grad()\n",
    "        output = net(feat_data, adj)\n",
    "        \n",
    "        loss = net.criterion(output[mini_batch], labels[mini_batch])\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "        if multi_class:\n",
    "            output[output > 0.5] = 1\n",
    "            output[output <= 0.5] = 0\n",
    "        else:\n",
    "            output = output.argmax(dim=1)\n",
    "\n",
    "        acc = f1_score(output[mini_batch].detach().cpu(), \n",
    "                       labels[mini_batch].detach().cpu(), average=\"micro\")\n",
    "        epoch_acc.append(acc)\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def inference(eval_model, feat_data, labels, test_data, device):\n",
    "    eval_model = eval_model.to(device)\n",
    "    mini_batch, adj = test_data    \n",
    "    output = eval_model(feat_data, adj)\n",
    "    loss = eval_model.criterion(output[mini_batch], labels[mini_batch]).item()\n",
    "    \n",
    "    if multi_class:\n",
    "        output[output > 0.5] = 1\n",
    "        output[output <= 0.5] = 0\n",
    "    else:\n",
    "        output = output.argmax(dim=1)\n",
    "        \n",
    "    acc = f1_score(output[mini_batch].detach().cpu(), \n",
    "                   labels[mini_batch].detach().cpu(), average=\"micro\")\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train without sampling\n",
    "\"\"\"\n",
    "\n",
    "def train_model(model, data_loader, note):\n",
    "    train_model = copy.deepcopy(model).to(device)\n",
    "    \n",
    "    results = ResultRecorder(note=note)\n",
    "\n",
    "    optimizer = optim.Adam(train_model.parameters())\n",
    "\n",
    "    tbar = trange(args.epoch_num, desc='Training Epochs')\n",
    "    for epoch in tbar:\n",
    "        # fetch train data \n",
    "        \n",
    "        sample_time_st = time.perf_counter()\n",
    "        train_data = data_loader.get_mini_batches(batch_size=args.batch_size)\n",
    "        sample_time = time.perf_counter() - sample_time_st\n",
    "        \n",
    "        compute_time_st = time.perf_counter()\n",
    "        train_loss, train_acc = sgd_step(train_model, optimizer, feat_data_th, labels_th, train_data, device)\n",
    "        compute_time = time.perf_counter() - compute_time_st\n",
    "        \n",
    "        epoch_train_loss = np.mean(train_loss)\n",
    "        epoch_train_acc = np.mean(train_acc)\n",
    "\n",
    "        valid_data = data_loader.get_valid_batch()\n",
    "        epoch_valid_loss, epoch_valid_acc = inference(train_model, feat_data_th, labels_th, valid_data, device)\n",
    "        tbar.set_postfix(loss=epoch_train_loss,\n",
    "                         val_loss=epoch_valid_loss,\n",
    "                         val_score=epoch_valid_acc)\n",
    "\n",
    "        results.update(epoch_train_loss, \n",
    "                       epoch_train_acc,\n",
    "                       epoch_valid_loss, \n",
    "                       epoch_valid_acc, \n",
    "                       train_model, sample_time=sample_time, compute_time=compute_time)\n",
    "\n",
    "\n",
    "    test_data = data_loader.get_test_batch()\n",
    "    epoch_test_loss, epoch_test_acc = inference(results.best_model, feat_data_th, labels_th, test_data, device)\n",
    "    results.test_loss = epoch_test_loss\n",
    "    results.test_acc = epoch_test_acc\n",
    "    print('Test_loss: %.4f | test_acc: %.4f' % (epoch_test_loss, epoch_test_acc))\n",
    "    \n",
    "    print('Average sampling time %.5fs, average computing time %.5fs'%\n",
    "          (np.mean(results.sample_time), np.mean(results.compute_time)))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " model = GCN(n_feat=feat_data.shape[1], \n",
    "                n_hid=args.nhid, \n",
    "                n_classes=num_classes, \n",
    "                n_layers=args.n_layers, \n",
    "                dropout=args.dropout, \n",
    "                criterion=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[288.28256916174786,\n",
       " 12.231014297738561,\n",
       " 3.4863095092493013,\n",
       " 1.179425685066999,\n",
       " 0.38728533034251805,\n",
       " 0.1374194308795775,\n",
       " 0.0651630906966079,\n",
       " 0.02355800005956585,\n",
       " 0.008672244773082441,\n",
       " 0.0032437887190805243,\n",
       " 0.001442599638868466]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_untrain = copy.deepcopy(model)\n",
    "compute_dM(model_untrain.to(device), F_mat, feat_data_th, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
